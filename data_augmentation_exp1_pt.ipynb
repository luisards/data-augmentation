{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "palestinian-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import nlpaug\n",
    "import nlpaug.augmenter.word as naw\n",
    "import keras\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "public-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list()\n",
    "file = open('stopwords_pt.txt',encoding='utf-8')\n",
    "for word in file.read().splitlines():\n",
    "    stop.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "original-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('OffComBR3.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "essential-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_w2v = naw.WordEmbsAug(\n",
    "    model_type='fasttext', model_path='cbow_s100.txt',\n",
    "    action=\"substitute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "academic-package",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sejam honestos aprovem o projeto original vamos acabar com esta farra no Brasil'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data.iloc[6]['tweet_text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "brutal-venture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Text:\n",
      "Sejam honestos aprovem o projeto original -vamos acabar com quauficada farra no Brasil\n",
      "Sejam honestos aprovem o projeto logográfica precisávamos acabar com esta farra no Brasil\n",
      "Sejam honestos aprovem o projeto arquivístico-musical vamos grassar com esta farra no Brasil\n",
      "Sejam honestos aprovem o projeto origial vamos acabar com esta palhaça no Brasil\n",
      "Sejam conscienciosos aprovem o projeto original vamos acabar com esta farra outubro.num Brasil\n"
     ]
    }
   ],
   "source": [
    "aug_w2v.aug_p=0.2\n",
    "print(\"Augmented Text:\")\n",
    "for ii in range(5):\n",
    "    augmented_text = aug_w2v.augment(text)\n",
    "    print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "domestic-cemetery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1033, 2)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "coastal-cooper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    831\n",
       "1    202\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "rotary-covering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train (1306, 2)\n",
      "Shape of Validation  (327, 2)\n"
     ]
    }
   ],
   "source": [
    "train,valid=train_test_split(data,test_size=0.20)\n",
    "print('Shape of Train',train.shape)\n",
    "print(\"Shape of Validation \",valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "automotive-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def augment_text(df,samples=300,pr=0.2):\n",
    "    aug_w2v.aug_p=pr\n",
    "    new_text=[]\n",
    "    \n",
    "    ##dropping samples from validation\n",
    "    df_n=df[df.target==1].reset_index(drop=True)\n",
    "\n",
    "    ## data augmentation loop\n",
    "    for i in tqdm(np.random.randint(0,len(df_n),samples)):\n",
    "        \n",
    "            text = df_n.iloc[i]['tweet_text']\n",
    "            augmented_text = aug_w2v.augment(text)\n",
    "            new_text.append(augmented_text)\n",
    "    \n",
    "    \n",
    "    ## dataframe\n",
    "    new=pd.DataFrame({'tweet_text':new_text,'target':1})\n",
    "    df=shuffle(df.append(new).reset_index(drop=True))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "immediate-repository",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [02:10<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "train = augment_text(train,samples=800)\n",
    "data = train.append(valid).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "planned-latest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    831\n",
       "1    802\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "entitled-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(data):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(data['tweet_text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet, language='portuguese') if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "moral-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2233/2233 [00:00<00:00, 3787.11it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus=create_corpus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "classical-isaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "929606it [01:33, 9896.47it/s] \n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('cbow_s100.txt',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "   \tvalues = line.strip().rsplit(' ')\n",
    "   \tword = values[0]\n",
    "   \tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "   \tembeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "close-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "saving-burden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 4651\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "medium-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 4651/4651 [00:00<00:00, 155134.58it/s]\n"
     ]
    }
   ],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embeddings_index.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "spanish-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "negative-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 50, 100)           465200    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 545,701\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 465,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "median-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = tweet_pad[:train.shape[0]],data['target'][:train.shape[0]]\n",
    "X_test,y_test= tweet_pad[train.shape[0]:],data['target'][train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "extraordinary-birmingham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "239/239 - 44s - loss: 0.5875 - accuracy: 0.6973 - val_loss: 0.6122 - val_accuracy: 0.6575\n",
      "Epoch 2/10\n",
      "239/239 - 41s - loss: 0.5570 - accuracy: 0.7219 - val_loss: 0.5781 - val_accuracy: 0.7064\n",
      "Epoch 3/10\n",
      "239/239 - 42s - loss: 0.5497 - accuracy: 0.7298 - val_loss: 0.5581 - val_accuracy: 0.7217\n",
      "Epoch 4/10\n",
      "239/239 - 44s - loss: 0.5358 - accuracy: 0.7282 - val_loss: 0.5460 - val_accuracy: 0.7401\n",
      "Epoch 5/10\n",
      "239/239 - 52s - loss: 0.5384 - accuracy: 0.7361 - val_loss: 0.5363 - val_accuracy: 0.7462\n",
      "Epoch 6/10\n",
      "239/239 - 44s - loss: 0.5352 - accuracy: 0.7429 - val_loss: 0.5280 - val_accuracy: 0.7492\n",
      "Epoch 7/10\n",
      "239/239 - 46s - loss: 0.5258 - accuracy: 0.7413 - val_loss: 0.5234 - val_accuracy: 0.7584\n",
      "Epoch 8/10\n",
      "239/239 - 42s - loss: 0.5229 - accuracy: 0.7466 - val_loss: 0.5134 - val_accuracy: 0.7676\n",
      "Epoch 9/10\n",
      "239/239 - 48s - loss: 0.5097 - accuracy: 0.7476 - val_loss: 0.5082 - val_accuracy: 0.7645\n",
      "Epoch 10/10\n",
      "239/239 - 44s - loss: 0.5068 - accuracy: 0.7602 - val_loss: 0.5038 - val_accuracy: 0.7706\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=8,epochs=10,validation_data=(X_test,y_test),verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "narrow-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=model.predict(X_test)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(327)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "frozen-pattern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7748299319727892\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_pre,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "promotional-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5425549773375861\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-oxide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
